import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

from src.ml.dino.config import DinoEnvConfig
from src.ml.dino.action_spec import DinoAction
from src.ml.model.dino_sl_model import DinoSLModel  

MODEL_PATH = "dino_sl_cnn.pt"

def main():
    env = DinoEnvConfig()

    data = np.load("dino_sl_dataset.npz")
    X = data["obs"]     # (N,obs,obs,1) uint8
    y = data["label"]   # (N,)

    # ì•ˆì „ ì²´í¬: env.obs_sizeì™€ ë°ì´í„° í¬ê¸° ì¼ì¹˜
    assert X.shape[1] == env.obs_size and X.shape[2] == env.obs_size, \
        f"dataset obs_size={X.shape[1:3]} != env.obs_size={env.obs_size}"

    # torch: (N,1,H,W) float
    X = torch.from_numpy(X).permute(0, 3, 1, 2).float() / 255.0
    y = torch.from_numpy(y).long()

    # âœ… ì§„ë‹¨: ë¼ë²¨ì´ ì‹¤ì œë¡œ ì–´ë–¤ ê°’ë“¤ë¡œ êµ¬ì„±ëëŠ”ì§€ í™•ì¸ (DUCK=2ê°€ ì•„ì˜ˆ ì—†ì„ ìˆ˜ ìˆìŒ)
    # ì˜ˆ) [0,1] ì´ë©´ DUCK(2) ìƒ˜í”Œì´ 0ê°œ â†’ ê¸°ì¡´ weight ê³„ì‚°ì´ shape=[2]ê°€ ë˜ì–´ ì—ëŸ¬ ë°œìƒ
    print("unique labels:", torch.unique(y).tolist())

    n = len(y)
    idx = torch.randperm(n)
    split = int(n * 0.9)
    tr, va = idx[:split], idx[split:]

    train_loader = DataLoader(TensorDataset(X[tr], y[tr]), batch_size=64, shuffle=True)
    val_loader = DataLoader(TensorDataset(X[va], y[va]), batch_size=256)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = DinoSLModel(n_actions=len(DinoAction)).to(device)

     # â­ Fine-tuning
    if os.path.exists(MODEL_PATH):
        print("ğŸ”¥ Loading existing model (fine-tuning)")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    else:
        print("â­ Training from scratch")


    # ë°ì´í„°ì— ì—†ëŠ” í´ë˜ìŠ¤(count=0)ëŠ” weightë¥¼ 0ìœ¼ë¡œ ë‘¬ì„œ(=í•™ìŠµì—ì„œ ë¬´ì‹œ) ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ ì§„í–‰
    num_classes = len(DinoAction)
    counts = torch.bincount(y, minlength=num_classes).float()  # âœ… minlengthë¡œ ê¸¸ì´ ë³´ì¥

    w = torch.zeros(num_classes, dtype=torch.float32)
    nonzero = counts > 0
    w[nonzero] = counts.sum() / counts[nonzero]               # ì—­ë¹„ìœ¨ ê°€ì¤‘ì¹˜
    w = w / (w[nonzero].mean() + 1e-9)                        # ìˆëŠ” í´ë˜ìŠ¤ë§Œ í‰ê·  1ë¡œ ì •ê·œí™”

    print("class counts:", counts.tolist(), "weights:", w.tolist())

    criterion = nn.CrossEntropyLoss(weight=w.to(device))

    opt = optim.Adam(model.parameters(), lr=1e-3)

    for epoch in range(1, 11):
        model.train()
        loss_sum = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            opt.step()
            loss_sum += loss.item()

        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                pred = model(xb).argmax(dim=1)
                correct += (pred == yb).sum().item()
                total += yb.numel()

        print(f"Epoch {epoch} | loss {loss_sum/len(train_loader):.4f} | val acc {correct/total:.3f}")

    torch.save(model.backbone.state_dict(), "dino_sl_cnn.pt")
    print("Saved model: dino_sl_cnn.pt")

if __name__ == "__main__":
    main()
