from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor

from src.ml.dino.config import DinoEnvConfig
from src.ml.dino.dino_env import DinoEnv
from src.ml.rl.policy import DinoFeatureExtractor


############################################
# env factory (ë©€í‹° í”„ë¡œì„¸ìŠ¤ìš©)
############################################
def make_env(rank: int):
    def _init():
        env = DinoEnv(DinoEnvConfig())
        env = Monitor(env)   # â­ reward / episode ê¸¸ì´ ìë™ ê¸°ë¡
        return env
    return _init


############################################
# main
############################################
def main():

    # â­â­â­ ë§¤ìš° ì¤‘ìš” â€” RL ì†ë„ 3~6ë°° ìƒìŠ¹
    N_ENVS = 4

    env = SubprocVecEnv([make_env(i) for i in range(N_ENVS)])

    ############################################
    # í‰ê°€ìš© env (best ëª¨ë¸ ì €ì¥ìš©)
    ############################################
    eval_env = SubprocVecEnv([make_env(999)])

    policy_kwargs = dict(
        features_extractor_class=DinoFeatureExtractor,
        features_extractor_kwargs=dict(features_dim=256),

        # CNN ì´í›„ MLP
        net_arch=dict(
            pi=[256, 256],
            vf=[256, 256],
        ),
    )

    model = PPO(
        "CnnPolicy",
        env,
        policy_kwargs=policy_kwargs,
        verbose=1,

        ############################
        # ğŸ”¥ Dinoì— ë§¤ìš° ì¢‹ì€ ì„¸íŒ…
        ############################
        n_steps=4096,        # â† ì¤‘ìš” (2048ë³´ë‹¤ ì•ˆì •ì )
        batch_size=256,
        learning_rate=2.5e-4,

        gamma=0.995,        # ì˜¤ë˜ ì‚´ì•„ë‚¨ëŠ” ì •ì±… í•™ìŠµ
        gae_lambda=0.98,

        clip_range=0.15,    # PPO ì•ˆì •í™”
        ent_coef=0.01,      # íƒí—˜ ì¦ê°€ â­â­â­

        device="cuda",
        tensorboard_log="./ppo_dino_tensorboard/",
    )

    ############################################
    # callbacks
    ############################################

    checkpoint_callback = CheckpointCallback(
        save_freq=50_000,
        save_path="./checkpoints/",
        name_prefix="ppo_dino",
    )

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        eval_freq=25_000,
        deterministic=True,
        render=False,
    )

    ############################################
    # train
    ############################################

    model.learn(
        total_timesteps=1_000_000,
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True,
    )

    model.save("ppo_dino_final")

    env.close()
    eval_env.close()


if __name__ == "__main__":
    main()
